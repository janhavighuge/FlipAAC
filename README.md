# Multi-Modal Emotion Recognition for Individuals with Disabilities

**Grant:** $5000  

## Project Focus  
Developed an **AI-driven application** designed to provide tailored language outputs for individuals with **speech-related disabilities** by integrating **computer vision** and **natural language processing (NLP)**. The system generates comprehensive language outputs by analyzing sentiment and combining speech and written inputs with a language model like **GPT-3**.

## Technologies Involved  
- **Computer Vision:** OpenCV, DeepFace (for face detection)  
- **AI Systems:** GPT-3.5, TextBlob (for text sentiment analysis), CNNs (for model training)  
- **Web App:** Flask, GitHub Actions (CI/CD), Azure Cloud  
- **Accessibility:** ASL (American Sign Language) input integration for enhanced accessibility  

## Project Goals  
- Generate accurate, dynamic language outputs by combining **sentiment analysis** with **speech** and **text inputs**.  
- Improve communication for individuals with disabilities through multi-modal interaction, ensuring accessibility.  

## Key Contributions  
- **Collaboration:** Worked alongside a team of 3 members to design and develop the system.  
- **Emotion Recognition System:** Integrated **OpenCV's DeepFace** for facial emotion detection and combined it with sentiment analysis using **GPT-3.5** and **TextBlob**.  
- **CI/CD Pipeline:** Built automated pipelines using **GitHub Actions**, ensuring efficient deployment and updates.  
- **Scalable Web App:** Deployed a secure, containerized **Flask app** on **Azure**, ensuring scalability and security for user interactions.  
- **Physiological Data Processing:** Processed and analyzed **EEG time-series data from 100+ users** using **signal processing and ML**, improving data insights by **30%**.  

## Future Plans  
Secured additional research funding for the acquisition of **IoT devices**, including an **EEG device**, to further enhance the system's ability to understand and analyze emotional states in real-time.  


